{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3144,
     "status": "ok",
     "timestamp": 1600689960609,
     "user": {
      "displayName": "Ming Lei",
      "photoUrl": "",
      "userId": "17751575944006759834"
     },
     "user_tz": -120
    },
    "id": "6-jRxAIxJtgL",
    "outputId": "319f3bb1-5d30-40e4-9471-bd859b068375"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import itertools as it\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "#import argparse\n",
    "import re\n",
    "import os\n",
    "import pickle\n",
    "import csv\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wd54uR4iKU-0"
   },
   "outputs": [],
   "source": [
    "def load_data_file(filename):\n",
    "    df = pd.read_excel(filename)\n",
    "    return df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OO6rs_9xKXxA"
   },
   "outputs": [],
   "source": [
    "ds = load_data_file('GBPAUD_3M.xlsx') #GBPAUD_Long_Small.xlsx  ;  GBPAUD_.xlsx; GBPAUD_3M.xlsx;GBPAUD_New_Small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 734,
     "status": "ok",
     "timestamp": 1600635122146,
     "user": {
      "displayName": "Ming Lei",
      "photoUrl": "",
      "userId": "17751575944006759834"
     },
     "user_tz": -120
    },
    "id": "aIYhW4eVKbCo",
    "outputId": "a38a2c30-3835-407a-b2d0-eb3d136a7d3c"
   },
   "outputs": [],
   "source": [
    "ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 784,
     "status": "ok",
     "timestamp": 1600635120198,
     "user": {
      "displayName": "Ming Lei",
      "photoUrl": "",
      "userId": "17751575944006759834"
     },
     "user_tz": -120
    },
    "id": "r--H6Sk3_gyD",
    "outputId": "46319cd5-fbf2-4763-ffe2-722c80b46dde"
   },
   "outputs": [],
   "source": [
    "type(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1270,
     "status": "ok",
     "timestamp": 1600635127007,
     "user": {
      "displayName": "Ming Lei",
      "photoUrl": "",
      "userId": "17751575944006759834"
     },
     "user_tz": -120
    },
    "id": "48wWxK0O_hix",
    "outputId": "2076deca-ed12-4db5-b399-9b7558e92806"
   },
   "outputs": [],
   "source": [
    "ds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "89OF7yRQKeAL"
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "  #inspired by lazy programmer\n",
    "    def __init__(self, state_dim, action_dim, size):\n",
    "        self.cur_state_buffer = np.zeros([size, state_dim], dtype=np.float32)\n",
    "        self.next_state_buffer = np.zeros([size, state_dim], dtype=np.float32)\n",
    "        self.actions_buffer = np.zeros(size, dtype=np.uint8)\n",
    "        self.rewards_buffer = np.zeros(size, dtype=np.float32)\n",
    "        self.done_buffer = np.zeros(size, dtype=np.uint8)\n",
    "        self.pointer, self.size, self.max_size = 0, 0, size\n",
    "\n",
    "    def save_experience(self, cur_state, action, reward, next_state, done):\n",
    "        self.cur_state_buffer[self.pointer] = cur_state\n",
    "        self.next_state_buffer[self.pointer] = next_state\n",
    "        self.actions_buffer[self.pointer] = action\n",
    "        self.rewards_buffer[self.pointer] = reward\n",
    "        self.done_buffer[self.pointer] = done\n",
    "        self.pointer = (self.pointer+1) % self.max_size\n",
    "        self.size = min(self.size+1, self.max_size)\n",
    "\n",
    "    def random_batch(self, batch_size=500):\n",
    "        idx = np.random.randint(0, self.size, size=batch_size)\n",
    "        return dict(s=self.cur_state_buffer[idx],\n",
    "                    s2=self.next_state_buffer[idx],\n",
    "                    a=self.actions_buffer[idx],\n",
    "                    r=self.rewards_buffer[idx],\n",
    "                    d=self.done_buffer[idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gYtC268JKldQ"
   },
   "outputs": [],
   "source": [
    "def create_scaler(env):\n",
    "    #scale the features\n",
    "    states = []\n",
    "    n = env.n_step\n",
    "    for x in range(n):\n",
    "        action = np.random.choice(env.action_space)\n",
    "        state, reward, done, info, _, _, _ = env.step(action)\n",
    "        states.append(state)\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    scl = StandardScaler()\n",
    "    scl.fit(states)\n",
    "    return scl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qbWqCdH2KoP7"
   },
   "outputs": [],
   "source": [
    "def create_directory(namedirectory):\n",
    "    if not os.path.exists(namedirectory):\n",
    "        os.makedirs(namedirectory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PyhxnclYiDk1"
   },
   "outputs": [],
   "source": [
    "class DQN_Agent(object):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.action_size = action_size\n",
    "        self.state_size = state_size\n",
    "        self.gamma = 0.95 # discount rate\n",
    "        self.epsilon = 1 # exploration rate\n",
    "        self.epsilon_min = 0.05\n",
    "        self.epsilon_decay = 0.99\n",
    "        self.learning_rate=0.001\n",
    "        #self.previous_target = np.random.rand()\n",
    "        self.model = self.DNN()\n",
    "        self.memory = ReplayBuffer(state_size, action_size, size=20000)\n",
    "\n",
    "    def DNN(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(64, input_dim = self.state_size, activation = 'relu'))\n",
    "        model.add(Dense(64, activation = 'relu'))\n",
    "        model.add(Dense(64, activation = 'relu'))\n",
    "        model.add(Dense(32, activation = 'relu'))\n",
    "        model.add(Dense(32, activation = 'relu'))\n",
    "        model.add(Dense(32, activation = 'relu'))\n",
    "        model.add(Dense(24, activation = 'relu'))\n",
    "        model.add(Dense(24, activation = 'relu'))\n",
    "        model.add(Dense(24, activation = 'relu'))\n",
    "        model.add(Dense(24, activation = 'relu'))\n",
    "        model.add(Dense(self.action_size,activation='linear'))\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "        print((model.summary()))\n",
    "        return model\n",
    "\n",
    "    def update_replay_memory(self, state, action, reward, next_state, done):\n",
    "        self.memory.save_experience(state, action, reward, next_state, done)\n",
    "\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon: #random selecion\n",
    "            #random selection\n",
    "            return np.random.choice(self.action_size)\n",
    "        act_values = self.model.predict(state)\n",
    "        '''print('state')\n",
    "        print(state)\n",
    "        print('act_values')\n",
    "        print(act_values)\n",
    "        '''\n",
    "        return np.argmax(act_values[0])\t# return the action with the higest predicted reward\n",
    "\n",
    "\n",
    "    def replay(self, batch_size=500):\n",
    "        # check wether there is enough data\n",
    "        if self.memory.size < batch_size:\n",
    "            return\n",
    "\n",
    "        #random batch\n",
    "        minibatch = self.memory.random_batch(batch_size)\n",
    "        states = minibatch['s']\n",
    "        actions = minibatch['a']\n",
    "        rewards = minibatch['r']\n",
    "        next_states = minibatch['s2']\n",
    "        done = minibatch['d']\n",
    "\n",
    "        # Calculate the tentative target: Q(s',a)\n",
    "\n",
    "        target = rewards + np.amax(self.model.predict(next_states), axis=1) *self.gamma *(1-done)\n",
    "\n",
    "        target_f = self.model.predict(states) #using cur_state\n",
    "        #only update the action number which has been predicted\n",
    "        target_f[np.arange(batch_size), actions] = target \n",
    "        # train the model\n",
    "        self.model.train_on_batch(states, target_f) \n",
    "\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "        \n",
    "        #self.previous_target = target\n",
    "        #return results\n",
    "\n",
    "\n",
    "    def load_model(self, filename):\n",
    "        self.model.load_weights(filename)\n",
    "\n",
    "\n",
    "    def save_model(self, filename):\n",
    "        self.model.save_weights(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "76TtgTMBiDkz"
   },
   "outputs": [],
   "source": [
    "class TradingEnv:\n",
    "\n",
    "    def __init__(self, dataset, initial_investment=100000):\n",
    "        self.n_step, self.state_dim = dataset.shape \n",
    "        self.dataset= dataset\n",
    "        self.initial_investment = initial_investment\n",
    "        self.has_expo = 0 #0 - no expo, 1 with long, -1 with short\n",
    "        self.comm = -8.87 # commission is fixed at 8.87 EUR per tarding one lot\n",
    "        self.EURAUD = 1.65 #fixed at 1.65\n",
    "        self.value = initial_investment\n",
    "        self.tradecount_long =0\n",
    "        self.transactioncost_long =0.0\n",
    "        self.tradecount_short =0\n",
    "        self.transactioncost_short =0.0\n",
    "        self.action_space = np.arange(3)\n",
    "        self.trx_cost = -30.0 \n",
    "        self.action_list = list(map(list, it.product([0, 1, 2], repeat=1)))\n",
    "        self.session_cost =0.0\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        self.cur_step = 0\n",
    "        self.initial_investment = initial_investment\n",
    "        cur_state = self.dataset[self.cur_step]\n",
    "        self.has_expo=0\n",
    "        return cur_state\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        #assert action in self.action_space\n",
    "        # get current value before performing the action\n",
    "        #prev_val = self._get_val()\n",
    "        # update price, i.e. go to the next day\n",
    "        cur_state = self.dataset[self.cur_step]\n",
    "        cur_state[0] = self.has_expo #get the cur exposure from last step\n",
    "        next_state = self.dataset[self.cur_step +1]\n",
    "        '''print('cur_state')\n",
    "        print(cur_state)\n",
    "        print('next state')\n",
    "        print(next_state)'''\n",
    "        \n",
    "        #simple risk management plociy\n",
    "        #maximal 5 longs or 5 shorts at at any time\n",
    "        if self.has_expo >= 5 and action==0:\n",
    "            action =1\n",
    "        elif self.has_expo <= -5 and action==2:\n",
    "            action=1\n",
    "        \n",
    "        #action 0 is to long\n",
    "        if action == 0 and self.has_expo >= 0: #to add one long\n",
    "            self.has_expo += 1 #for next step\n",
    "            reward =next_state[1] * self.has_expo + self.trx_cost * cost_scaler #reward is the sum of price delta and trx cost\n",
    "            self.transactioncost_long += self.trx_cost #stats\n",
    "            self.tradecount_long +=1 #stats\n",
    "\n",
    "        elif action == 0 and self.has_expo <0: #to long, but with existing short, it will only close the short to no exposure\n",
    "            self.has_expo = 0 #it closes the existing short position\n",
    "            reward =0 \n",
    "\n",
    "        #action 2 is to short\n",
    "        elif action ==2 and self.has_expo <=0: #it adds a short position with transaction cost\n",
    "            self.has_expo += -1 #negative expo\n",
    "            reward =next_state[1]* self.has_expo + self.trx_cost * cost_scaler #reward is the sum of price delta and trx cost\n",
    "            self.transactioncost_short += self.trx_cost #stats\n",
    "            self.tradecount_short +=1 #stats\n",
    "\n",
    "        elif action ==2 and self.has_expo >0: #with existing long position, short action will close the long\n",
    "            self.has_expo = 0 #close the existing position\n",
    "            reward = 0 \n",
    "\n",
    "\n",
    "        #action 1 is to hold cash or existing position\n",
    "        elif action==1 and self.has_expo==0: #hold cash, no reward\n",
    "            reward = 0\n",
    "\n",
    "        elif action ==1 and self.has_expo != 0: #hold existing exposure, get price delta as reward\n",
    "            reward = next_state[1] * self.has_expo\n",
    "        \n",
    "        self.value += reward\n",
    "        self.cur_step += 1\n",
    "        self.dataset[self.cur_step][0] = self.has_expo #for the next step\n",
    "        next_state[0]= self.has_expo\n",
    "        done = self.cur_step == self.n_step - 1\n",
    "        info = {'cur_val':self.value}\n",
    "        '''print('cur_state')\n",
    "        print(cur_state)\n",
    "        print('next state')\n",
    "        print(next_state)'''\n",
    "        return next_state, reward, done, info, self.transactioncost_long, self.transactioncost_short, self.has_expo\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sN3NNOuSiDk7"
   },
   "outputs": [],
   "source": [
    "def run_episode(agent, env, is_train):\n",
    "    # note: after transforming states are already 1xD\n",
    "    state = env.reset()\n",
    "    state = scaler.transform([state])\n",
    "    done = False\n",
    "    n= 0\n",
    "    has_expo = 0\n",
    "    env.initial_investment = initial_investment\n",
    "    env.tradecount_long = 0\n",
    "    env.transactioncost_long =0.0\n",
    "    env.tradecount_short = 0\n",
    "    env.transactioncost_short =0.0\n",
    "    env.value = initial_investment\n",
    "    #print(env.n_step)\n",
    "    ls =[]\n",
    "    \n",
    "    while not done:\n",
    "        lsr=[]\n",
    "        action = agent.act(state)\n",
    "        \n",
    "        next_state, reward, done, info, tc_l, tc_s, expo = env.step(action)\n",
    "        next_state = scaler.transform([next_state])\n",
    "        result=0\n",
    "        lsr.append(action)\n",
    "        lsr.append(reward)\n",
    "        lsr.append(tc_l)\n",
    "        lsr.append(tc_s)\n",
    "        lsr.append(expo)\n",
    "        ls.append(lsr)\n",
    "        if is_train == 'train':\n",
    "            \n",
    "            agent.update_replay_memory(state, action, reward, next_state, done)\n",
    "            results = agent.replay(batch_size)\n",
    "        state = next_state\n",
    "        n+=1\n",
    "        #print(str(n)+ \"; action:\" + str(action) + \" ; reward: \" +str(reward))\n",
    "    '''\n",
    "    with open('test_result_fullcost_09-29.csv', 'w', newline='') as f:\n",
    "        wr=csv.writer(f)\n",
    "        wr.writerows(ls)\n",
    "    '''\n",
    "    return info['cur_val'], env.transactioncost_long, env.tradecount_long, env.transactioncost_short, env.tradecount_short"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 568
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 750,
     "status": "ok",
     "timestamp": 1600636448221,
     "user": {
      "displayName": "Ming Lei",
      "photoUrl": "",
      "userId": "17751575944006759834"
     },
     "user_tz": -120
    },
    "id": "P6bXnSNBiDk4",
    "outputId": "103dc28c-0b15-4cc9-9bbd-9993d3e61704"
   },
   "outputs": [],
   "source": [
    "#train the model\n",
    "num_episodes = 5\n",
    "batch_size = 500\n",
    "initial_investment = 100000\n",
    "cost_scaler =1.0\n",
    "n_timesteps, _ = ds.shape\n",
    "train_data = ds[:100]\n",
    "env = TradingEnv(train_data, initial_investment)\n",
    "state_size = env.state_dim\n",
    "action_size = len(env.action_space)\n",
    "agent = DQN_Agent(state_size, action_size)\n",
    "scaler = create_scaler(env)\n",
    "portfolio_value = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 735,
     "status": "ok",
     "timestamp": 1600636451292,
     "user": {
      "displayName": "Ming Lei",
      "photoUrl": "",
      "userId": "17751575944006759834"
     },
     "user_tz": -120
    },
    "id": "Xys8G86HVEmC",
    "outputId": "9620eeaa-0537-4b40-b8ef-8308137626de"
   },
   "outputs": [],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 30442613,
     "status": "ok",
     "timestamp": 1600671650904,
     "user": {
      "displayName": "Ming Lei",
      "photoUrl": "",
      "userId": "17751575944006759834"
     },
     "user_tz": -120
    },
    "id": "CIT2R-syiDk8",
    "outputId": "0f3e24cc-39b8-43fe-abd0-85df230ca366"
   },
   "outputs": [],
   "source": [
    "    #start tarining \n",
    "    traintest ='train'\n",
    "    cost_scaler=1.0 # to adjust the trx cost, min 0, max 1.0\n",
    "    initial_epsilon =1.0\n",
    "    for e in range(num_episodes):\n",
    "        if cost_scaler >=1.0:\n",
    "            cost_scaler=1.0\n",
    "        # load trained weights\n",
    "        agent.load_model('dqn.h5') #load the trained weights only if continuing from previous training\n",
    "        agent.epsilon = initial_epsilon\n",
    "        agent.epsilon_min =0.05\n",
    "        agent.epsilon_decay =0.999\n",
    "        t0 = datetime.now()\n",
    "        val, transactioncost_long, tradecount_long, transactioncost_short, tradecount_short = run_episode(agent, env, traintest)\n",
    "        dt = datetime.now() - t0\n",
    "        print(f\"train: e: {e}/{num_episodes}, cost:{cost_scaler}, init_eps:{initial_epsilon}, val: {val:.2f}, cost_long: {transactioncost_long:.2f} , cnt_long: {tradecount_long}, cost_short: {transactioncost_short:.2f} , cnt_short: {tradecount_short}, dur: {dt}\")\n",
    "        agent.save_model('dqn.h5')\n",
    "        initial_epsilon *=0.95\n",
    "        #cost_scaler +=0.05 #only needed when stepping up the trx cost\n",
    "\n",
    "    # save the model if training, but not saving if it's test model\n",
    "    if traintest == 'train':\n",
    "        agent.save_model('dqn.h5')\n",
    "        with open('scaler.pkl', 'wb') as f:\n",
    "            pickle.dump(scaler, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j67jw-16PybV"
   },
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data=ds[10000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# play the game num_episodes times\n",
    "cost_scaler=1.0\n",
    "initial_investment=100000\n",
    "\n",
    "#load the scaler based on training data\n",
    "with open('scaler.pkl', 'rb') as f:\n",
    "    scaler = pickle.load(f)\n",
    "\n",
    "env_test = TradingEnv(test_data, initial_investment)\n",
    "state_size = env_test.state_dim\n",
    "action_size = len(env_test.action_space)\n",
    "agent_test = DQN_Agent(state_size, action_size)\n",
    "    \n",
    "agent_test.load_model('dqn.h5')\n",
    "#agent_test.epsilon =1.0 #for random strategy\n",
    "agent_test.epsilon = 0.0 #testing, it's zerothen derministic\n",
    "agent_test.epsilon_min =0.0\n",
    "agent_test.epsilon_decay=0.0\n",
    "\n",
    "t0_test = datetime.now()\n",
    "val_test, transactioncost_long_test, tradecount_long_test, transactioncost_short_test, tradecount_short_test= run_episode(agent_test, env_test, 'test')\n",
    "dt_test = datetime.now() - t0_test\n",
    "print(f\"test: val: {val_test:.2f}, costLong: {transactioncost_long_test:.2f} , cntLong: {tradecount_long_test}, costShort: {transactioncost_short_test:.2f} , cntShort: {tradecount_short_test} ,dur: {dt_test}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyP0UW/NL3YTD4aWgzWVv3rq",
   "collapsed_sections": [],
   "name": "Copy of FX Trading 2020-09-19 2.ipynb",
   "provenance": [
    {
     "file_id": "1PxclyedNn8eq86Lkko-KB2z0p14p-Mtu",
     "timestamp": 1600689969949
    },
    {
     "file_id": "1vvu-TvBqxhJqfqUhgVrJOm38JP3bgO6z",
     "timestamp": 1600508687494
    },
    {
     "file_id": "10lesg-qhNgM1EbkoqqAK3ObXuzCZOrVj",
     "timestamp": 1600378771199
    },
    {
     "file_id": "1z67XjwXN_Oh6z1MSiqgUXWGLtvjIEd9g",
     "timestamp": 1600070286980
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
